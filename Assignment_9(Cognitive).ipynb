{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoGbR1cV9eme/AtLwnYOX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakharraj11/Cognitive-Computing/blob/main/Assignment_9(Cognitive).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "0-y5tTme1Gdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "CgmdrM0f6UL6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lshK7aqyB-WB",
        "outputId": "679eaa56-33f7-4bb2-c0b0-c8d3fa59c566"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "K7wmYnkm03Fj"
      },
      "outputs": [],
      "source": [
        "s = \"\"\"Space exploration represents humanity's boldest ambition to reach beyond our planet and discover the mysteries of the cosmos. The technological innovations developed for space missions have\n",
        "       consistently transformed our daily lives, from satellite communications to advanced materials. Scientists and engineers around the world collaborate on ambitious projects that push the boundaries\n",
        "       of what we thought possible decades ago. Looking up at the stars reminds us of our relatively small place in the universe, yet inspires us to dream bigger. The future of space exploration holds\n",
        "       promise for discovering new worlds, resources, and perhaps even answering the ultimate question of whether we are alone in the universe.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = str.lower(s) # converts the given string to lower case\"\n",
        "clean_s = s.translate(str.maketrans(\"\",\"\",string.punctuation)) # punctutaions removed by matching characters present in string.punctution with s and if there is as match it is replaced with \"\"\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBg1RD1N5SFr",
        "outputId": "4a7dd2ee-88e9-4bf8-c5f6-fe951233764d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "space exploration represents humanity's boldest ambition to reach beyond our planet and discover the mysteries of the cosmos. the technological innovations developed for space missions have \n",
            "       consistently transformed our daily lives, from satellite communications to advanced materials. scientists and engineers around the world collaborate on ambitious projects that push the boundaries \n",
            "       of what we thought possible decades ago. looking up at the stars reminds us of our relatively small place in the universe, yet inspires us to dream bigger. the future of space exploration holds \n",
            "       promise for discovering new worlds, resources, and perhaps even answering the ultimate question of whether we are alone in the universe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize into words and sentences\n",
        "sentences = sent_tokenize(s)\n",
        "words = word_tokenize(clean_s)\n",
        "print(sentences)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEd_OkBWBPJS",
        "outputId": "4c1aaf2b-f730-47a0-fba8-ae1b75b94bc8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"space exploration represents humanity's boldest ambition to reach beyond our planet and discover the mysteries of the cosmos.\", 'the technological innovations developed for space missions have \\n       consistently transformed our daily lives, from satellite communications to advanced materials.', 'scientists and engineers around the world collaborate on ambitious projects that push the boundaries \\n       of what we thought possible decades ago.', 'looking up at the stars reminds us of our relatively small place in the universe, yet inspires us to dream bigger.', 'the future of space exploration holds \\n       promise for discovering new worlds, resources, and perhaps even answering the ultimate question of whether we are alone in the universe.']\n",
            "['space', 'exploration', 'represents', 'humanitys', 'boldest', 'ambition', 'to', 'reach', 'beyond', 'our', 'planet', 'and', 'discover', 'the', 'mysteries', 'of', 'the', 'cosmos', 'the', 'technological', 'innovations', 'developed', 'for', 'space', 'missions', 'have', 'consistently', 'transformed', 'our', 'daily', 'lives', 'from', 'satellite', 'communications', 'to', 'advanced', 'materials', 'scientists', 'and', 'engineers', 'around', 'the', 'world', 'collaborate', 'on', 'ambitious', 'projects', 'that', 'push', 'the', 'boundaries', 'of', 'what', 'we', 'thought', 'possible', 'decades', 'ago', 'looking', 'up', 'at', 'the', 'stars', 'reminds', 'us', 'of', 'our', 'relatively', 'small', 'place', 'in', 'the', 'universe', 'yet', 'inspires', 'us', 'to', 'dream', 'bigger', 'the', 'future', 'of', 'space', 'exploration', 'holds', 'promise', 'for', 'discovering', 'new', 'worlds', 'resources', 'and', 'perhaps', 'even', 'answering', 'the', 'ultimate', 'question', 'of', 'whether', 'we', 'are', 'alone', 'in', 'the', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removal of Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(stop_words)\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou3Qo1liCbCv",
        "outputId": "15114e65-239a-4cc7-9896-737225ea52b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yourselves', 'wouldn', 'very', 'a', \"that'll\", 'do', 'hers', 'of', 'they', 'is', 'other', 'our', 'myself', 'if', 'isn', 'ain', 'after', 'against', 'd', \"wasn't\", \"shan't\", 'm', 'through', 'he', \"he'd\", 'being', 'more', \"needn't\", 'over', 'wasn', 'who', 'won', 'me', 'on', 'are', 've', \"don't\", 'ourselves', 'than', 'up', \"she'd\", 'about', \"you're\", 'own', \"weren't\", 'this', \"we're\", 'doesn', 'for', 'have', \"she'll\", 'be', 'such', 'by', \"couldn't\", 'were', \"i've\", \"wouldn't\", 'now', 'above', 'further', 'under', \"won't\", 'we', \"i'd\", 'whom', 'don', 'same', 'itself', 'ma', \"she's\", 'll', 'yours', \"they'll\", 'and', 'once', 'which', 'below', 'into', 'but', 'all', 'theirs', 'haven', \"you'll\", \"they'd\", 'she', 'doing', 'in', \"mustn't\", 'your', \"isn't\", 'at', 's', \"didn't\", 'nor', 'or', 'the', \"we'll\", 'them', 'does', \"should've\", 'just', 'ours', 'what', 'herself', 'didn', 'should', 'when', \"hasn't\", 'it', 'mightn', 'weren', 'had', \"they've\", 'has', 'no', \"we'd\", \"it'll\", 'there', 'from', 'himself', 'any', \"mightn't\", 'can', 'my', 'off', 'mustn', 'each', 'yourself', \"they're\", 'an', 'him', \"i'll\", 'while', 'with', 'as', 'not', 'shan', 'that', 're', 'couldn', 'hadn', 'to', 'out', 'been', 'these', 'needn', \"doesn't\", 'few', \"he's\", 'why', 'where', 'y', 'aren', 'her', \"aren't\", 'did', 'so', \"he'll\", 'then', 'you', \"shouldn't\", 'here', 'until', \"you've\", 'again', 'o', 'its', 'will', 'i', 'only', 'most', 'how', 'his', 'those', 'down', \"haven't\", 'having', \"hadn't\", 'their', 'between', 'was', 'hasn', 'both', \"it'd\", 'shouldn', 'themselves', \"it's\", \"we've\", \"you'd\", 'some', 'because', 'am', 'too', \"i'm\", 'during', 't', 'before'}\n",
            "['space', 'exploration', 'represents', 'humanitys', 'boldest', 'ambition', 'reach', 'beyond', 'planet', 'discover', 'mysteries', 'cosmos', 'technological', 'innovations', 'developed', 'space', 'missions', 'consistently', 'transformed', 'daily', 'lives', 'satellite', 'communications', 'advanced', 'materials', 'scientists', 'engineers', 'around', 'world', 'collaborate', 'ambitious', 'projects', 'push', 'boundaries', 'thought', 'possible', 'decades', 'ago', 'looking', 'stars', 'reminds', 'us', 'relatively', 'small', 'place', 'universe', 'yet', 'inspires', 'us', 'dream', 'bigger', 'future', 'space', 'exploration', 'holds', 'promise', 'discovering', 'new', 'worlds', 'resources', 'perhaps', 'even', 'answering', 'ultimate', 'question', 'whether', 'alone', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word frequency lab count\n",
        "from collections import Counter\n",
        "word_freq = Counter(filtered_words)\n",
        "most_common = word_freq.most_common(10)\n",
        "print(word_freq)\n",
        "print(most_common)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MOx8g1nDAz8",
        "outputId": "02ebd0c3-3919-467a-fb10-0c4b381c5d50"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'space': 3, 'exploration': 2, 'us': 2, 'universe': 2, 'represents': 1, 'humanitys': 1, 'boldest': 1, 'ambition': 1, 'reach': 1, 'beyond': 1, 'planet': 1, 'discover': 1, 'mysteries': 1, 'cosmos': 1, 'technological': 1, 'innovations': 1, 'developed': 1, 'missions': 1, 'consistently': 1, 'transformed': 1, 'daily': 1, 'lives': 1, 'satellite': 1, 'communications': 1, 'advanced': 1, 'materials': 1, 'scientists': 1, 'engineers': 1, 'around': 1, 'world': 1, 'collaborate': 1, 'ambitious': 1, 'projects': 1, 'push': 1, 'boundaries': 1, 'thought': 1, 'possible': 1, 'decades': 1, 'ago': 1, 'looking': 1, 'stars': 1, 'reminds': 1, 'relatively': 1, 'small': 1, 'place': 1, 'yet': 1, 'inspires': 1, 'dream': 1, 'bigger': 1, 'future': 1, 'holds': 1, 'promise': 1, 'discovering': 1, 'new': 1, 'worlds': 1, 'resources': 1, 'perhaps': 1, 'even': 1, 'answering': 1, 'ultimate': 1, 'question': 1, 'whether': 1, 'alone': 1})\n",
            "[('space', 3), ('exploration', 2), ('us', 2), ('universe', 2), ('represents', 1), ('humanitys', 1), ('boldest', 1), ('ambition', 1), ('reach', 1), ('beyond', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and Lemmitization\n",
        "1. Take the tokenized words from Question 1 (after stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "RlbAbR_U8_Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use of PorterStemmer\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(words) for words in filtered_words]\n",
        "print(singles)"
      ],
      "metadata": {
        "id": "FwOi9C2jA5AW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b2decf-14dd-4ed3-fc4b-143ac431bd60"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'explor', 'repres', 'human', 'boldest', 'ambit', 'reach', 'beyond', 'planet', 'discov', 'mysteri', 'cosmo', 'technolog', 'innov', 'develop', 'space', 'mission', 'consist', 'transform', 'daili', 'live', 'satellit', 'commun', 'advanc', 'materi', 'scientist', 'engin', 'around', 'world', 'collabor', 'ambiti', 'project', 'push', 'boundari', 'thought', 'possibl', 'decad', 'ago', 'look', 'star', 'remind', 'us', 'rel', 'small', 'place', 'univers', 'yet', 'inspir', 'us', 'dream', 'bigger', 'futur', 'space', 'explor', 'hold', 'promis', 'discov', 'new', 'world', 'resourc', 'perhap', 'even', 'answer', 'ultim', 'question', 'whether', 'alon', 'univers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use of LancasterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "st = LancasterStemmer()\n",
        "lansingles = [st.stem(words) for words in filtered_words]\n",
        "print(lansingles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eti-LucBGR6j",
        "outputId": "7ec90401-6441-4ffc-ca18-ae5c1278c50d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spac', 'expl', 'repres', 'hum', 'boldest', 'ambit', 'reach', 'beyond', 'planet', 'discov', 'mystery', 'cosmo', 'technolog', 'innov', 'develop', 'spac', 'miss', 'consist', 'transform', 'dai', 'liv', 'satellit', 'commun', 'adv', 'mat', 'sci', 'engin', 'around', 'world', 'collab', 'amb', 'project', 'push', 'bound', 'thought', 'poss', 'decad', 'ago', 'look', 'star', 'remind', 'us', 'rel', 'smal', 'plac', 'univers', 'yet', 'inspir', 'us', 'dream', 'big', 'fut', 'spac', 'expl', 'hold', 'prom', 'discov', 'new', 'world', 'resourc', 'perhap', 'ev', 'answ', 'ultim', 'quest', 'wheth', 'alon', 'univers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use of WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "lamsingles = [wnl.lemmatize(words) for words in filtered_words]\n",
        "print(lamsingles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exIGcuzmGpjL",
        "outputId": "ce29cf38-ea32-407e-b02e-4819f8467514"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'exploration', 'represents', 'humanity', 'boldest', 'ambition', 'reach', 'beyond', 'planet', 'discover', 'mystery', 'cosmos', 'technological', 'innovation', 'developed', 'space', 'mission', 'consistently', 'transformed', 'daily', 'life', 'satellite', 'communication', 'advanced', 'material', 'scientist', 'engineer', 'around', 'world', 'collaborate', 'ambitious', 'project', 'push', 'boundary', 'thought', 'possible', 'decade', 'ago', 'looking', 'star', 'reminds', 'u', 'relatively', 'small', 'place', 'universe', 'yet', 'inspires', 'u', 'dream', 'bigger', 'future', 'space', 'exploration', 'hold', 'promise', 'discovering', 'new', 'world', 'resource', 'perhaps', 'even', 'answering', 'ultimate', 'question', 'whether', 'alone', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Splitinng\n",
        "1. Take their original text from Question 1.\n",
        "2. Use regular expressions to:\n",
        "\n",
        "  a. Extract all words with more than 5 letters.\n",
        "\n",
        "  b. Extract all numbers (if any exist in their text).\n",
        "\n",
        "  c. Extract all capitalized words.\n",
        "3. Use text splittinng techniques to:\n",
        "\n",
        "  a. Split the text into words containing only alphabets (removing digits and special characters).\n",
        "\n",
        "  b. Extract words starting with a vowel."
      ],
      "metadata": {
        "id": "w8JRZc7a9ERZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "XYi0aQOm9f6s"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting words with more than 5 letters using re\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', s)\n",
        "print(long_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA-V5VxCHjYP",
        "outputId": "8e29c366-ca50-4e38-c459-16135207b094"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exploration', 'represents', 'humanity', 'boldest', 'ambition', 'beyond', 'planet', 'discover', 'mysteries', 'cosmos', 'technological', 'innovations', 'developed', 'missions', 'consistently', 'transformed', 'satellite', 'communications', 'advanced', 'materials', 'Scientists', 'engineers', 'around', 'collaborate', 'ambitious', 'projects', 'boundaries', 'thought', 'possible', 'decades', 'Looking', 'reminds', 'relatively', 'universe', 'inspires', 'bigger', 'future', 'exploration', 'promise', 'discovering', 'worlds', 'resources', 'perhaps', 'answering', 'ultimate', 'question', 'whether', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting numbers\n",
        "nums = re.findall('-?\\b\\d+\\.\\d+\\b', s)\n",
        "print(nums)\n",
        "#- means negative, \\b means this should be a seperate word/token and should not be in between of another words, \\d+ means all possible numbers even in continuation,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFbfue2POA-8",
        "outputId": "9a51c6f2-c39e-48fd-e269-fe232e7bff5b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find all capitalized words\n",
        "caps = re.findall(r'\\b[A-Z][a-z]*\\b',s)\n",
        "print(caps)\n",
        "#\\b marks the start of the word, [A-Z] marks the starting character can be in this range, next set of * chars can be in [a-zA-Z] in this range."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8KbmrlqP2cV",
        "outputId": "94c10773-a5d9-4e09-d7cb-636d8dbac51d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Space', 'The', 'Scientists', 'Looking', 'The']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into words using re\n",
        "words = clean_s.split()\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCf3SblRLfJ",
        "outputId": "4fad748c-2867-4cad-874c-5296adb2dbfb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'exploration', 'represents', 'humanitys', 'boldest', 'ambition', 'to', 'reach', 'beyond', 'our', 'planet', 'and', 'discover', 'the', 'mysteries', 'of', 'the', 'cosmos', 'the', 'technological', 'innovations', 'developed', 'for', 'space', 'missions', 'have', 'consistently', 'transformed', 'our', 'daily', 'lives', 'from', 'satellite', 'communications', 'to', 'advanced', 'materials', 'scientists', 'and', 'engineers', 'around', 'the', 'world', 'collaborate', 'on', 'ambitious', 'projects', 'that', 'push', 'the', 'boundaries', 'of', 'what', 'we', 'thought', 'possible', 'decades', 'ago', 'looking', 'up', 'at', 'the', 'stars', 'reminds', 'us', 'of', 'our', 'relatively', 'small', 'place', 'in', 'the', 'universe', 'yet', 'inspires', 'us', 'to', 'dream', 'bigger', 'the', 'future', 'of', 'space', 'exploration', 'holds', 'promise', 'for', 'discovering', 'new', 'worlds', 'resources', 'and', 'perhaps', 'even', 'answering', 'the', 'ultimate', 'question', 'of', 'whether', 'we', 'are', 'alone', 'in', 'the', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting words starting with a vowel\n",
        "vowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b',clean_s)\n",
        "print(vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anC18cB6SIeN",
        "outputId": "fa4b2d07-fe1b-41a1-abf7-d5b04cb6c711"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exploration', 'ambition', 'our', 'and', 'of', 'innovations', 'our', 'advanced', 'and', 'engineers', 'around', 'on', 'ambitious', 'of', 'ago', 'up', 'at', 'us', 'of', 'our', 'in', 'universe', 'inspires', 'us', 'of', 'exploration', 'and', 'even', 'answering', 'ultimate', 'of', 'are', 'alone', 'in', 'universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "1. Take original text from Question 1.\n",
        "2. Write a custom tokenization function that:\n",
        "\n",
        "  a. Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").\n",
        "\n",
        "  b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
        "\n",
        "  c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "  should remain as is).\n",
        "\n",
        "3. Use Regex Substitutions (re.sub) to:\n",
        "\n",
        "  a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "\n",
        "  b. Replace URLs with '<URL>' placeholder.\n",
        "\n",
        "  c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "  'PHONE' placeholder."
      ],
      "metadata": {
        "id": "r5ZPy9209gPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def custom_tokenizer(text):\n",
        "    # Step 1: Replace sensitive entities using regex\n",
        "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)  # Emails\n",
        "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)          # URLs\n",
        "    text = re.sub(r'\\+?\\d{1,3}[ -]?\\d{10}|\\d{3}-\\d{3}-\\d{4}', 'PHONE', text)  # Phone numbers\n",
        "\n",
        "    # Step 2: Define a custom regex pattern for tokenization\n",
        "        # \\b\\w+(?:'\\w+)?\\b     Words with optional contractions (e.g., isn't)\n",
        "        # \\b\\d+\\.\\d+\\b         Decimal numbers\n",
        "        # \\b\\d+\\b              Integer numbers\n",
        "        # \\b\\w+(?:-\\w+)+\\b     Hyphenated words (e.g., state-of-the-art)\n",
        "\n",
        "    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b|\\b\\d+\\.\\d+\\b\\|\\b\\d+\\b|\\b\\w+(?:-\\w+)+\\b\", text)\n",
        "    return tokens\n",
        "\n",
        "tokens = custom_tokenizer(s)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDX7TEEA90VP",
        "outputId": "beae7dfc-d7c1-472c-8e6d-b4b9578fd6a2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'exploration', 'represents', \"humanity's\", 'boldest', 'ambition', 'to', 'reach', 'beyond', 'our', 'planet', 'and', 'discover', 'the', 'mysteries', 'of', 'the', 'cosmos', 'the', 'technological', 'innovations', 'developed', 'for', 'space', 'missions', 'have', 'consistently', 'transformed', 'our', 'daily', 'lives', 'from', 'satellite', 'communications', 'to', 'advanced', 'materials', 'scientists', 'and', 'engineers', 'around', 'the', 'world', 'collaborate', 'on', 'ambitious', 'projects', 'that', 'push', 'the', 'boundaries', 'of', 'what', 'we', 'thought', 'possible', 'decades', 'ago', 'looking', 'up', 'at', 'the', 'stars', 'reminds', 'us', 'of', 'our', 'relatively', 'small', 'place', 'in', 'the', 'universe', 'yet', 'inspires', 'us', 'to', 'dream', 'bigger', 'the', 'future', 'of', 'space', 'exploration', 'holds', 'promise', 'for', 'discovering', 'new', 'worlds', 'resources', 'and', 'perhaps', 'even', 'answering', 'the', 'ultimate', 'question', 'of', 'whether', 'we', 'are', 'alone', 'in', 'the', 'universe']\n"
          ]
        }
      ]
    }
  ]
}